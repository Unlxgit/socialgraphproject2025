{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53ed18da",
   "metadata": {},
   "source": [
    "# Filter, clean and summarize debate\n",
    "\n",
    "This notebook serves as a helper in the preprocessing, to make file operations on the raw debates quicker, as some laptops don't like opening 20000 seperate files. Additionally, debates that are split into multiple child debates are removed. No flex but my working station can loop and load the ~20000 json files in 5 seconds (:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b6e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f54d7-3572-40b3-bbab-fde32a8a4b31",
   "metadata": {},
   "source": [
    "Functions to clean debate texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c04c5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(text):\n",
    "    return re.sub(r\"<.*?>\", \"\", text)\n",
    "\n",
    "def normalize_whitespace(text):\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = clean_html(text)\n",
    "    text = normalize_whitespace(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe97e677",
   "metadata": {},
   "source": [
    "Function for cleaning and joining all json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4c3e407",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_debate_files(path):\n",
    "    all_debates_list = []\n",
    "    \n",
    "    for file in os.listdir(path):\n",
    "        # ignore directories\n",
    "        if os.path.isdir(file): \n",
    "            continue\n",
    "        \n",
    "        full_path = os.path.join(path, file)\n",
    "        with open(full_path, 'r', encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "    \n",
    "        debate_data = {}\n",
    "    \n",
    "        if \"Overview\" not in data:\n",
    "            continue\n",
    "    \n",
    "        # Collecting overall data\n",
    "        debate_data[\"Id\"]       = data[\"Overview\"][\"Id\"]\n",
    "        debate_data[\"ExtId\"]    = data[\"Overview\"][\"ExtId\"]\n",
    "        debate_data[\"Title\"]    = data[\"Overview\"][\"Title\"]\n",
    "        debate_data[\"Date\"]     = data[\"Overview\"][\"Date\"]\n",
    "        debate_data[\"Location\"] = data[\"Overview\"][\"Location\"]\n",
    "\n",
    "        # Ignoring debates with no content\n",
    "        if (\"Items\" not in data):\n",
    "            continue\n",
    "        \n",
    "        # Collecting all contributions\n",
    "        debate_data[\"Interactions\"] = []\n",
    "        \n",
    "        for interaction in data[\"Items\"]:\n",
    "            if (interaction[\"MemberId\"]) and (interaction[\"ItemType\"] == \"Contribution\"):\n",
    "                # Clean interactions text\n",
    "                Value = preprocess_text(interaction[\"Value\"])\n",
    "                \n",
    "                interaction_data = {\n",
    "                    \"ItemId\"          : interaction[\"ItemId\"],\n",
    "                    \"ExternalId\"      : interaction[\"ExternalId\"],\n",
    "                    \"MemberId\"        : interaction[\"MemberId\"],\n",
    "                    \"Value\"           : Value,\n",
    "                    \"OrderInSection\"  : interaction[\"OrderInSection\"],\n",
    "                    \"AttributedTo\"    : interaction[\"AttributedTo\"]\n",
    "                }\n",
    "                debate_data[\"Interactions\"].append(interaction_data)     \n",
    "\n",
    "        # only gather debates with more than 1 contribution\n",
    "        if (len(debate_data[\"Interactions\"]) > 1):\n",
    "            all_debates_list.append(debate_data)\n",
    "\n",
    "    return all_debates_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cdcd54b-fc04-4d39-a4c0-09ccdc25df24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_debates_list = preprocess_debate_files(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d775d0",
   "metadata": {},
   "source": [
    "Number of debates left to analyse after for interaction validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee06284a-c428-44e2-910c-09114b49434b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14719"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_debates_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f886cc",
   "metadata": {},
   "source": [
    "Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36910cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_debates_list, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
