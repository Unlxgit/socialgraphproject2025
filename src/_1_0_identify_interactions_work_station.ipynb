{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e94ec5b",
   "metadata": {},
   "source": [
    "# Intentifiing Interactions\n",
    "\n",
    "As in our analysis we want to analyse how Members of Parliament in the UK house of commons interact with each other we have to filter out irrelevant data or to phrase it the other way around, we have to identify interactions. \n",
    "\n",
    "In order to achieve this we leveraged recent advancements in LLMs. We tried out different LLMs to classify whether two contributions are interacting or not. Almost all Open source small scale LLMs tended to overclassify interactions. \n",
    "\n",
    "The only model that was found and could run in a reasonable timeframe was the gpt-oss-safeguard-8k:20b model by OpenAI, a policy fine tuned reasoning model. This model performed to our satisfaction in a small qualitative analysis. \n",
    "\n",
    "In order to run this yourself, you at best have a graphics card with at least 16 gb of VRAM, running on cpu is possible but unbarebly slow. Install ollama and pull and run the gpt-oss-safeguard:20b. It is then exposed locally and can be queried from this notebook.\n",
    "\n",
    "Its actually our favorite part of our network analysis, but Sune said, that he doesn't like methods sections. So oh well.\n",
    "\n",
    "This notebook can be rerun as required, as we remove already analysed debated from the files that are still open for analysis. If you want the raw extracted lists of interactions you can reach out to s252890@dtu.dk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from ollama import chat\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41129db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.json\", 'r', encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef4d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_analysed_debates = [name.replace(\".json\", \"\").split(\"_\")[1] for name in os.listdir(\"interactions\") if name.endswith(\".json\")]\n",
    "\n",
    "data = [debate for debate in data if debate['ExtId'] not in list_of_analysed_debates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c839b1-eb05-47ac-99da-7d4eded7239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.strptime('17-12-2019', '%d-%m-%Y')\n",
    "end = datetime.strptime('30-05-2024', '%d-%m-%Y')\n",
    "\n",
    "data = [debate for debate in data if start <= datetime.fromisoformat(debate['Date']) <= end]\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f304e0",
   "metadata": {},
   "source": [
    "This is our prompt for our model, to identify, whether two concecutive contributions can be seen as interacting or not. It provides the mode with pare minimum context and the policy, which it should follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d869dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT =  \"\"\"\n",
    "You will be given a list of two contributions in a debate in the UK House of Commons. \n",
    "Each contribution is a dictionary with:\n",
    "- Speaker: the name of the speaker\n",
    "- Utterance: the text that the speaker said\n",
    "\n",
    "Decide if the second contribution is interacting with the first contribution. \n",
    "A contribution is considered interacting if the second speaker, anywhere in their utterance:\n",
    "1. Directly addresses the first speaker.\n",
    "2. Builds upon a point made by the first speaker, or\n",
    "3. Asks a question referring to the first speaker's contribution.\n",
    "\n",
    "Answer only \"Yes\" if it is interacting, otherwise answer \"No\". \n",
    "Do not provide any explanations or extra text.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30834f49",
   "metadata": {},
   "source": [
    "Looping over all interactions in all debates to evaluate, whether they are interacting or not. If they interact, the file is safed to the interactions directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584916e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = \"interactions\"\n",
    "keys_to_keep = [\"AttributedTo\", \"Value\"]\n",
    "rename_keys = {\"Value\": \"Utterance\", \"AttributedTo\": \"Speaker\"}\n",
    "\n",
    "\n",
    "for debate in data:\n",
    "    captured_interactions = []\n",
    "    print(\"Debate id:\", debate[\"ExtId\"])\n",
    "\n",
    "    for i in range(len(debate[\"Interactions\"])-1):\n",
    "        interaction_1 = debate[\"Interactions\"][i]\n",
    "        interaction_2 = debate[\"Interactions\"][i+1]\n",
    "\n",
    "        # Keep only the essential data\n",
    "        interaction_1_filtered = {rename_keys[k]: interaction_1[k] for k in keys_to_keep}\n",
    "        interaction_2_filtered = {rename_keys[k]: interaction_2[k] for k in keys_to_keep}\n",
    "\n",
    "        interactions = f\"{json.dumps([interaction_1_filtered, interaction_2_filtered], indent=2)}\"\n",
    "\n",
    "        # Prompt\n",
    "        response = chat(\n",
    "            model=\"gpt-oss-safeguard-8k:20b\", # gpt-oss-safeguard:20b\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": interactions}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # If there's interaction between A and B --> save data\n",
    "        if response['message']['content'].strip().lower() == \"yes\":\n",
    "            print(\"Yes\")\n",
    "            captured_interactions.append(\n",
    "                {            \n",
    "                    \"speaker1_member_id\": interaction_1[\"MemberId\"],\n",
    "                    \"speaker2_member_id\": interaction_2[\"MemberId\"],\n",
    "                    \"speaker1_order\"    : interaction_1[\"OrderInSection\"],\n",
    "                    \"speaker2_order\"    : interaction_2[\"OrderInSection\"],\n",
    "                    \"speaker1_text\"     : interaction_1[\"Value\"],\n",
    "                    \"speaker2_text\"     : interaction_2[\"Value\"],\n",
    "                    \"debate_id\"         : debate[\"ExtId\"],\n",
    "                    \"debate_date\"       : debate[\"Date\"],\n",
    "                    \"Location\"          : debate[\"Location\"],\n",
    "                    \"Title\"             : debate[\"Title\"]\n",
    "                }\n",
    "            )\n",
    "\n",
    "        elif response['message']['content'].strip().lower() == \"no\":\n",
    "            print(\"No\")\n",
    "\n",
    "        # Check if there's an unexpected response\n",
    "        else:\n",
    "            print(\"\\n\\nSOMETHING WENT WRONG!\\n\\n\")\n",
    "            print(\"response:\", response['message']['content'])\n",
    "\n",
    "\n",
    "    file = \"debate_\" + debate[\"ExtId\"] + \".json\"\n",
    "    \n",
    "    with open(os.path.join(path, file), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(captured_interactions, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
